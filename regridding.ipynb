{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "206b715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import h5py\n",
    "import re\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d317cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Folder paths containing raw files ---\n",
    "SMAP_DIR = \"./Inputs/smap\"      \n",
    "RAIN_DIR = \"./Inputs/Rainfall\"\n",
    "TEMP_DIR = \"./Inputs/temperature\"\n",
    "\n",
    "# --- Output ---\n",
    "OUTPUT_DIR = \"./output\"\n",
    "OUTPUT_PARQUET = os.path.join(OUTPUT_DIR, \"odisha_merged_tabular.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876cb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Odisha bounding box \n",
    "LAT_MIN, LAT_MAX = 17.5, 23.0\n",
    "LON_MIN, LON_MAX = 81.0, 87.5\n",
    "\n",
    "def subset_odisha(lats, lons):\n",
    "    mask = (lats >= LAT_MIN) & (lats <= LAT_MAX) & (lons >= LON_MIN) & (lons <= LON_MAX)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d989cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filepath):\n",
    "    basename = os.path.basename(filepath)\n",
    "    # Pattern : YYYYMMDD \n",
    "    m = re.search(r'(?<!\\d)(\\d{4})(0[1-9]|1[0-2])(0[1-9]|[12]\\d|3[01])(?!\\d)', basename)\n",
    "    if m:\n",
    "        return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3))).date()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ea3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMAP_CONFIG = {\n",
    "    \"sm_dataset\":  \"Geophysical_Data/sm_surface\",           # AM pass\n",
    "    \"lat_dataset\": \"cell_lat\",\n",
    "    \"lon_dataset\": \"cell_lon\",\n",
    "    \"fill_value\":  -9999.0,\n",
    "}\n",
    "\n",
    "# --- Rain NetCDF variable names (IMD / GPM / CHIRPS typical names) ---\n",
    "RAIN_CONFIG = {\n",
    "    \"rain_var\": \"RAINFALL\",          # could be 'precip', 'precipitation', 'rain', 'tp', etc.\n",
    "    \"lat_var\":  \"LATITUDE\",         # could be 'latitude', 'LAT', etc.\n",
    "    \"lon_var\":  \"LONGITUDE\",         # could be 'longitude', 'LON', etc.\n",
    "    \"time_var\": \"TIME\",        # could be 'TIME', 'date', etc.\n",
    "}\n",
    "\n",
    "# --- Temperature .grd variable names (IMD / CRU / Berkeley typical names) ---\n",
    "TEMP_CONFIG = {\n",
    "    \"temp_var\": \"temp\",        # could be 'tmax', 'tmin', 'tmp', 'temperature', etc.\n",
    "    \"lat_var\":  \"lat\",         # could be 'latitude'\n",
    "    \"lon_var\":  \"lon\",         # could be 'longitude'\n",
    "    \"time_var\": \"time\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdac40e",
   "metadata": {},
   "source": [
    "# Reading and Processing SMAP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2e977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_smap_h5(filepath):  \n",
    "    file_date = extract_date_from_filename(filepath)\n",
    "    if file_date is None:\n",
    "        print(f\"  ⚠ Could not parse date from: {os.path.basename(filepath)}, skipping.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with h5py.File(filepath, \"r\") as f:\n",
    "        sm  = f[SMAP_CONFIG[\"sm_dataset\"]][:]\n",
    "        lat = f[SMAP_CONFIG[\"lat_dataset\"]][:]\n",
    "        lon = f[SMAP_CONFIG[\"lon_dataset\"]][:]\n",
    "\n",
    "    sm = sm.astype(np.float64)\n",
    "    sm[sm == SMAP_CONFIG[\"fill_value\"]] = np.nan\n",
    "\n",
    "    # Flatten (SMAP L4 grids are 2D arrays)\n",
    "    lat_flat = lat.ravel()\n",
    "    lon_flat = lon.ravel()\n",
    "    sm_flat  = sm.ravel()\n",
    "\n",
    "    # Subset to Odisha bounding box\n",
    "    mask = subset_odisha(lat_flat, lon_flat)\n",
    "    lat_sub = lat_flat[mask]\n",
    "    lon_sub = lon_flat[mask]\n",
    "    sm_sub  = sm_flat[mask]\n",
    "\n",
    "    # Drop NaN SM values (no-data pixels)\n",
    "    valid = ~np.isnan(sm_sub)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date\":          file_date,\n",
    "        \"lat\":           np.round(lat_sub[valid], 4),\n",
    "        \"lon\":           np.round(lon_sub[valid], 4),\n",
    "        \"soil_moisture\": np.round(sm_sub[valid], 6),\n",
    "    })\n",
    "\n",
    "    print(f\"  ✓ SMAP  {file_date}  →  {len(df):>6,} pixels\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c6b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_smap(folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"**\", \"*.h5\"), recursive=True))\n",
    "    if not files:\n",
    "        print(f\"  ℹ No .h5 files found in {folder}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"  SMAP (.h5)  — {len(files)} file(s) in {folder}\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    dfs = [read_smap_h5(f) for f in files]\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61264b",
   "metadata": {},
   "source": [
    "## READ RAIN .nc FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb241a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rain_nc(filepath):\n",
    "    ds = xr.open_dataset(filepath)\n",
    "\n",
    "    # --- Get variable names from config\n",
    "    rain_var = RAIN_CONFIG[\"rain_var\"]\n",
    "    lat_var  = RAIN_CONFIG[\"lat_var\"]\n",
    "    lon_var  = RAIN_CONFIG[\"lon_var\"]\n",
    "    time_var = RAIN_CONFIG[\"time_var\"]\n",
    "\n",
    "    # Fallback auto-detection\n",
    "    all_vars = list(ds.data_vars) + list(ds.coords)\n",
    "    if rain_var not in ds.data_vars:\n",
    "        candidates = [v for v in ds.data_vars if v not in (\"lat\", \"lon\", \"latitude\", \"longitude\", \"time\")]\n",
    "        if candidates:\n",
    "            rain_var = candidates[0]\n",
    "            print(f\"    ↳ Auto-detected rain variable: '{rain_var}'\")\n",
    "\n",
    "    if lat_var not in ds.coords and lat_var not in ds.dims:\n",
    "        for candidate in [\"latitude\", \"LAT\", \"Latitude\", \"lat\"]:\n",
    "            if candidate in all_vars:\n",
    "                lat_var = candidate\n",
    "                break\n",
    "\n",
    "    if lon_var not in ds.coords and lon_var not in ds.dims:\n",
    "        for candidate in [\"longitude\", \"LON\", \"Longitude\", \"lon\"]:\n",
    "            if candidate in all_vars:\n",
    "                lon_var = candidate\n",
    "                break\n",
    "\n",
    "    if time_var not in ds.coords and time_var not in ds.dims:\n",
    "        for candidate in [\"TIME\", \"time\", \"date\", \"Date\"]:\n",
    "            if candidate in all_vars:\n",
    "                time_var = candidate\n",
    "                break\n",
    "\n",
    "    # --- Subset to Odisha ---\n",
    "    lats = ds[lat_var].values\n",
    "    lons = ds[lon_var].values\n",
    "\n",
    "    # Handle ascending / descending lat\n",
    "    lat_sel = sorted([LAT_MIN, LAT_MAX])\n",
    "    lon_sel = sorted([LON_MIN, LON_MAX])\n",
    "\n",
    "    if lats[0] > lats[-1]:  # descending\n",
    "        ds_sub = ds.sel(**{lat_var: slice(lat_sel[1], lat_sel[0]),\n",
    "                           lon_var: slice(lon_sel[0], lon_sel[1])})\n",
    "    else:\n",
    "        ds_sub = ds.sel(**{lat_var: slice(lat_sel[0], lat_sel[1]),\n",
    "                           lon_var: slice(lon_sel[0], lon_sel[1])})\n",
    "        \n",
    "    records = []\n",
    "    for t in ds_sub[time_var].values:\n",
    "        ts = pd.Timestamp(t).date()\n",
    "        slice_2d = ds_sub[rain_var].sel(**{time_var: t})\n",
    "        lats_2d, lons_2d = np.meshgrid(ds_sub[lat_var].values,ds_sub[lon_var].values, indexing=\"ij\")\n",
    "        rain_flat = slice_2d.values.ravel()\n",
    "        lat_flat  = lats_2d.ravel()\n",
    "        lon_flat  = lons_2d.ravel()\n",
    "        \n",
    "        valid = np.isfinite(rain_flat)\n",
    "        records.append(pd.DataFrame({\n",
    "            \"date\":     ts,\n",
    "            \"lat\":      np.round(lat_flat[valid], 4),\n",
    "            \"lon\":      np.round(lon_flat[valid], 4),\n",
    "            \"rainfall\": np.round(rain_flat[valid], 4),\n",
    "            }))\n",
    "    print(f\"  ✓ Rain  {ts}  →  {valid.sum():>6,} pixels\")\n",
    "    ds.close()\n",
    "    return pd.concat(records, ignore_index=True) if records else pd.DataFrame()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5853599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_rain(folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"**\", \"*.nc\"), recursive=True))\n",
    "    if not files:\n",
    "        print(f\" No .nc files found in {folder}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"  RAIN (.nc)  — {len(files)} file(s) in {folder}\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    dfs = [read_rain_nc(f) for f in files]\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80631af3",
   "metadata": {},
   "source": [
    "## READ TEMPERATURE .grd FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacf3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_temp_grd(filepath):\n",
    "    nx, ny = 31, 31\n",
    "    lon_start, lat_start = 67.5, 7.5\n",
    "    resolution = 1.0\n",
    "\n",
    "    # --- Extract year from filename (e.g. \"Maxtemp_MaxT_2020.GRD\" → 2020) ---\n",
    "    basename = os.path.basename(filepath)\n",
    "    year_match = re.search(r'(\\d{4})', basename)\n",
    "    year = int(year_match.group(1))\n",
    "    start_date = datetime(year, 1, 1)\n",
    "\n",
    "    filesize = os.path.getsize(filepath)\n",
    "    n_expected = nx * ny\n",
    "    n_timesteps = filesize // (n_expected * 4)  # float32 = 4 bytes\n",
    "\n",
    "    data = np.fromfile(filepath, dtype=np.float32)\n",
    "\n",
    "    lons = np.arange(lon_start, lon_start + nx * resolution, resolution)[:nx]\n",
    "    lats = np.arange(lat_start, lat_start + ny * resolution, resolution)[:ny]\n",
    "    lons_2d, lats_2d = np.meshgrid(lons, lats)\n",
    "\n",
    "    records = []\n",
    "    for t in range(n_timesteps):\n",
    "        offset = t * n_expected\n",
    "        data_2d = data[offset:offset + n_expected].reshape(ny, nx)\n",
    "\n",
    "        data_2d[(data_2d > 60) | (data_2d < -60)] = np.nan\n",
    "\n",
    "        temp_flat = data_2d.ravel()\n",
    "        lat_flat  = lats_2d.ravel()\n",
    "        lon_flat  = lons_2d.ravel()\n",
    "\n",
    "        mask = subset_odisha(lat_flat, lon_flat) & np.isfinite(temp_flat)\n",
    "\n",
    "        current_date = start_date + timedelta(days=t)\n",
    "\n",
    "        records.append(pd.DataFrame({\n",
    "            \"date\":        current_date,\n",
    "            \"lat\":         np.round(lat_flat[mask], 4),\n",
    "            \"lon\":         np.round(lon_flat[mask], 4),\n",
    "            \"temperature\": np.round(temp_flat[mask], 4),\n",
    "        }))\n",
    "    return pd.concat(records, ignore_index=True) if records else pd.DataFrame()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d07ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_temp(folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"**\", \"*.grd\"), recursive=True) +\n",
    "                   glob.glob(os.path.join(folder, \"**\", \"*.GRD\"), recursive=True))\n",
    "    if not files:\n",
    "        print(f\"  ℹ No .grd files found in {folder}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"  TEMP (.grd)  — {len(files)} file(s) in {folder}\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    dfs = [read_temp_grd(f) for f in files]\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372e01b",
   "metadata": {},
   "source": [
    "# Mergeing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f547b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_fill(df_base, df_other, value_col, tolerance_deg):\n",
    "    if df_other.empty:\n",
    "        df_base[value_col] = np.nan\n",
    "        return df_base\n",
    "\n",
    "    other_dates = set(df_other[\"date\"].unique())\n",
    "    results = []\n",
    "\n",
    "    for date, grp_base in df_base.groupby(\"date\"):\n",
    "        grp_base = grp_base.copy()\n",
    "\n",
    "        if date not in other_dates:\n",
    "            grp_base[value_col] = np.nan\n",
    "            results.append(grp_base)\n",
    "            continue\n",
    "\n",
    "        grp_other = df_other[df_other[\"date\"] == date]\n",
    "\n",
    "        tree = cKDTree(grp_other[[\"lat\", \"lon\"]].values)\n",
    "        dists, idxs = tree.query(grp_base[[\"lat\", \"lon\"]].values, k=1)\n",
    "\n",
    "        vals = grp_other[value_col].values[idxs]\n",
    "        vals[dists > tolerance_deg] = np.nan  # too far → skip\n",
    "        grp_base[value_col] = vals\n",
    "\n",
    "        matched = np.sum(dists <= tolerance_deg)\n",
    "        print(f\"    {value_col:<14} {date}  →  {matched:>6,}/{len(grp_base):,} matched\")\n",
    "\n",
    "        results.append(grp_base)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a0bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save(df_sm, df_rain, df_temp, output_path):\n",
    "    \"\"\"\n",
    "    Merge the three datasets on (date, lat, lon) and save to CSV.\n",
    "\n",
    "    Strategy:\n",
    "      • Each dataset lives on its own grid (different resolutions).\n",
    "      • We do an OUTER merge so NO pixels are lost.\n",
    "      • Where grids don't overlap exactly, the unmatched variable is NaN.\n",
    "      • For a nearest-neighbor spatial merge, see the optional section below.\n",
    "    \"\"\"\n",
    "    for df in [df_sm, df_rain, df_temp]:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df[\"lat\"]  = df[\"lat\"].astype(float)\n",
    "        df[\"lon\"]  = df[\"lon\"].astype(float)\n",
    "  \n",
    "    # Rainfall: IMD 0.25° grid → tolerance ~0.18° (half-diagonal of 0.25° cell)\n",
    "    merged = nn_fill(df_sm, df_rain, \"rainfall\", tolerance_deg=0.18)\n",
    "    print(f\"  After rainfall join: {merged['rainfall'].notna().sum():,} matched\")\n",
    "\n",
    "    # Temperature: IMD 1.0° grid → tolerance ~0.72° (half-diagonal of 1.0° cell)\n",
    "    merged = nn_fill(merged, df_temp, \"temperature\", tolerance_deg=0.72)\n",
    "    print(f\"  After temperature join: {merged['temperature'].notna().sum():,} matched\")\n",
    "\n",
    "    # Sort\n",
    "    merged = merged.sort_values([\"date\", \"lat\", \"lon\"],\n",
    "                                ascending=[True, False, True]).reset_index(drop=True)\n",
    "\n",
    "    # --- Save ---\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    merged.to_parquet(output_path, index=False)\n",
    "\n",
    "    print(f\"\\n  ✅ Saved: {output_path}\")\n",
    "    print(f\"     Rows       : {len(merged):,}\")\n",
    "    print(f\"     Columns    : {list(merged.columns)}\")\n",
    "    print(f\"     Date range : {merged['date'].min()} → {merged['date'].max()}\")\n",
    "    print(f\"     Lat range  : {merged['lat'].min():.4f} → {merged['lat'].max():.4f}\")\n",
    "    print(f\"     Lon range  : {merged['lon'].min():.4f} → {merged['lon'].max():.4f}\")\n",
    "    print(f\"     Rainfall fill  : {merged['rainfall'].notna().mean()*100:.1f}%\")\n",
    "    print(f\"     Temp fill      : {merged['temperature'].notna().mean()*100:.1f}%\")\n",
    "\n",
    "    # Per-variable CSVs\n",
    "    if not df_sm.empty:\n",
    "        path = output_path.replace(\".parquet\", \"_soil_moisture.parquet\")\n",
    "        df_sm.sort_values([\"date\", \"lat\", \"lon\"]).to_parquet(path, index=False)\n",
    "        print(f\"     + {path}\")\n",
    "    if not df_rain.empty:\n",
    "        path = output_path.replace(\".parquet\", \"_rainfall.parquet\")\n",
    "        df_rain.sort_values([\"date\", \"lat\", \"lon\"]).to_parquet(path, index=False)\n",
    "        print(f\"     + {path}\")\n",
    "    if not df_temp.empty:\n",
    "        path = output_path.replace(\".parquet\", \"_temperature.parquet\")\n",
    "        df_temp.sort_values([\"date\", \"lat\", \"lon\"]).to_parquet(path, index=False)\n",
    "        print(f\"     + {path}\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4af534",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88340bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "  SMAP (.h5)  — 7 file(s) in ./Inputs/smap\n",
      "────────────────────────────────────────────────────────────\n",
      "  ✓ SMAP  2020-01-02  →   3,867 pixels\n",
      "  ✓ SMAP  2020-01-13  →   3,867 pixels\n",
      "  ✓ SMAP  2020-01-14  →   3,867 pixels\n",
      "  ✓ SMAP  2024-03-08  →   3,867 pixels\n",
      "  ✓ SMAP  2024-03-09  →   3,867 pixels\n",
      "  ✓ SMAP  2025-12-25  →   3,867 pixels\n",
      "  ✓ SMAP  2025-12-26  →   3,867 pixels\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "  RAIN (.nc)  — 6 file(s) in ./Inputs/Rainfall\n",
      "────────────────────────────────────────────────────────────\n",
      "  ✓ Rain  2020-12-31  →     464 pixels\n",
      "  ✓ Rain  2021-12-31  →     464 pixels\n",
      "  ✓ Rain  2022-12-31  →     464 pixels\n",
      "  ✓ Rain  2023-12-31  →     464 pixels\n",
      "    ↳ Auto-detected rain variable: 'rf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AHRC\\Irrigation_Git\\Lag_adjustment\\.venv\\Lib\\site-packages\\xarray\\coding\\times.py:215: SerializationWarning: Ambiguous reference date string: 1-1-1 00:00:00. The first value is assumed to be the year hence will be padded with zeros to remove the ambiguity (the padded reference date string is: 0001-1-1 00:00:00). To remove this message, remove the ambiguity by padding your reference date strings with zeros.\n",
      "  ref_date = _ensure_padded_year(ref_date)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Rain  2024-12-31  →     464 pixels\n",
      "  ✓ Rain  2026-01-02  →     464 pixels\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "  TEMP (.grd)  — 12 file(s) in ./Inputs/temperature\n",
      "────────────────────────────────────────────────────────────\n",
      "    rainfall       2020-01-02 00:00:00  →   3,826/3,867 matched\n",
      "    rainfall       2020-01-13 00:00:00  →   3,826/3,867 matched\n",
      "    rainfall       2020-01-14 00:00:00  →   3,826/3,867 matched\n",
      "    rainfall       2024-03-08 00:00:00  →   3,826/3,867 matched\n",
      "    rainfall       2024-03-09 00:00:00  →   3,826/3,867 matched\n",
      "    rainfall       2025-12-25 00:00:00  →   3,826/3,867 matched\n",
      "    rainfall       2025-12-26 00:00:00  →   3,826/3,867 matched\n",
      "  After rainfall join: 26,782 matched\n",
      "    temperature    2020-01-02 00:00:00  →   3,867/3,867 matched\n",
      "    temperature    2020-01-13 00:00:00  →   3,867/3,867 matched\n",
      "    temperature    2020-01-14 00:00:00  →   3,867/3,867 matched\n",
      "    temperature    2024-03-08 00:00:00  →   3,867/3,867 matched\n",
      "    temperature    2024-03-09 00:00:00  →   3,867/3,867 matched\n",
      "    temperature    2025-12-25 00:00:00  →   3,867/3,867 matched\n",
      "    temperature    2025-12-26 00:00:00  →   3,867/3,867 matched\n",
      "  After temperature join: 27,069 matched\n",
      "\n",
      "  ✅ Saved: ./output\\odisha_merged_tabular.parquet\n",
      "     Rows       : 27,069\n",
      "     Columns    : ['date', 'lat', 'lon', 'soil_moisture', 'rainfall', 'temperature']\n",
      "     Date range : 2020-01-02 00:00:00 → 2025-12-26 00:00:00\n",
      "     Lat range  : 17.5296 → 22.9413\n",
      "     Lon range  : 81.0840 → 87.4326\n",
      "     Rainfall fill  : 98.9%\n",
      "     Temp fill      : 100.0%\n",
      "     + ./output\\odisha_merged_tabular_soil_moisture.parquet\n",
      "     + ./output\\odisha_merged_tabular_rainfall.parquet\n",
      "     + ./output\\odisha_merged_tabular_temperature.parquet\n"
     ]
    }
   ],
   "source": [
    "df_sm   = read_all_smap(SMAP_DIR)\n",
    "df_rain = read_all_rain(RAIN_DIR)\n",
    "df_temp = read_all_temp(TEMP_DIR)\n",
    "\n",
    "# --- Merge & save ---\n",
    "merged = merge_and_save(df_sm, df_rain, df_temp, OUTPUT_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bffc128e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3867 points, 0 differ from reference\n",
      "2024: 3867 points, 0 differ from reference\n",
      "2025: 3867 points, 0 differ from reference\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"./output/odisha_merged_tabular.parquet\")\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "groups = df.groupby(\"year\")[[\"lat\", \"lon\"]].apply(lambda g: set(zip(g[\"lat\"], g[\"lon\"])))\n",
    "\n",
    "# Check if all years have the same set of coordinates\n",
    "ref = groups.iloc[0]\n",
    "for year, coords in groups.items():\n",
    "    diff = coords.symmetric_difference(ref)\n",
    "    print(f\"{year}: {len(coords)} points, {len(diff)} differ from reference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
